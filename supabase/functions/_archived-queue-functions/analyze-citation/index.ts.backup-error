import { serve } from 'https://deno.land/std@0.168.0/http/server.ts'
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}

interface AnalyzeCitationRequest {
  query_id: string
  citation_url: string
  citation_position: number
  query_text: string
  keyword: string
  brand_name: string
  brand_domain: string
  competitors: any[]
}

// Helper function to escape regex special characters
function escapeRegex(string: string): string {
  return string.replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
}

// Safe function to find mentions with context
function findMentionsWithContext(content: string, searchTerm: string, contextLength: number = 100): string[] {
  const mentions: string[] = [];
  
  if (!searchTerm || !content) {
    return mentions;
  }
  
  try {
    // Escape the search term to make it safe for regex
    const escapedTerm = escapeRegex(searchTerm);
    
    // Create a safe regex pattern
    const pattern = new RegExp(`(.{0,${contextLength}})\\b${escapedTerm}\\b(.{0,${contextLength}})`, 'gi');
    
    let match;
    while ((match = pattern.exec(content)) !== null) {
      mentions.push(match[0]);
    }
  } catch (error) {
    console.error(`Error finding mentions for "${searchTerm}":`, error);
  }
  
  return mentions;
}

// Domain knowledge store to remember which domains need premium options
const knownDifficultDomains = new Map<string, number>();

// Function to determine if a domain likely needs premium options based on past experience
function needsPremiumOptions(url: string): number {
  try {
    const domain = new URL(url).hostname;
    return knownDifficultDomains.get(domain) || 1; // Default to basic tier (1)
  } catch {
    return 1; // Default to basic tier
  }
}

// Function to remember if a domain needed premium options
function rememberDomainDifficulty(url: string, requiredTier: number) {
  try {
    const domain = new URL(url).hostname;

    // Only update if the new tier is higher than what we knew before
    const currentTier = knownDifficultDomains.get(domain) || 0;
    if (requiredTier > currentTier) {
      console.log(`Remembering that domain ${domain} requires tier ${requiredTier}`);
      knownDifficultDomains.set(domain, requiredTier);
    }
  } catch (e) {
    console.error(`Error remembering domain difficulty: ${e.message}`);
  }
}

// List of known domains that typically require premium handling
const KNOWN_PROBLEM_DOMAINS = [
  'g2.com',
  'capterra.com',
  'softwareadvice.com',
  'trustradius.com',
  'techradar.com',
  'pcmag.com',
  'gartner.com',
  'getapp.com',
  'sourceforge.net',
  'cnet.com',
  'trustpilot.com'
];

// Initialize known difficult domains with software review sites
KNOWN_PROBLEM_DOMAINS.forEach(domain => {
  knownDifficultDomains.set(domain, 3); // Assume premium proxy needed (tier 3)
});

// Enhanced crawlPage function with fallback options and domain knowledge
async function crawlPage(url: string, attempt: number = 0): Promise<string> {
  const apiKey = Deno.env.get('SCRAPINGBEE_API_KEY')
  if (!apiKey) {
    console.error('ScrapingBee API key not found in environment')
    throw new Error('ScrapingBee API key not configured')
  }

  // If this is the first attempt, check if we already know this domain needs premium options
  if (attempt === 0) {
    attempt = needsPremiumOptions(url);
    console.log(`Starting with tier ${attempt} for URL: ${url} (based on domain knowledge)`);
  }

  // Cap at attempt 4 (stealth proxy is the highest tier)
  attempt = Math.min(attempt, 4);

  console.log(`Attempting to crawl URL: ${url} (tier ${attempt})`);

  // Validate URL
  try {
    new URL(url);
  } catch (e) {
    console.error(`Invalid URL provided: ${url}`);
    throw new Error(`Invalid URL format: ${url}`);
  }

  // Configure parameters based on attempt number
  let params: URLSearchParams;
  let creditCost = 1; // Track credit cost for analytics

  switch(attempt) {
    case 1:
      // First attempt: Basic crawl (1 credit)
      params = new URLSearchParams({
        api_key: apiKey,
        url: url,
        render_js: 'false',
        premium_proxy: 'false',
        country_code: 'us',
        device: 'desktop',
        return_page_source: 'true',
        block_ads: 'true',
        block_resources: 'false',
        wait: '0',
        timeout: '30000'
      });
      creditCost = 1;
      break;

    case 2:
      // Second attempt: JavaScript rendering (5 credits)
      params = new URLSearchParams({
        api_key: apiKey,
        url: url,
        render_js: 'true',
        premium_proxy: 'false',
        country_code: 'us',
        device: 'desktop',
        return_page_source: 'true',
        block_ads: 'true',
        wait: '2000',
        timeout: '30000'
      });
      creditCost = 5;
      break;

    case 3:
      // Third attempt: Premium proxy (10-25 credits)
      params = new URLSearchParams({
        api_key: apiKey,
        url: url,
        render_js: 'true',
        premium_proxy: 'true',
        country_code: 'us',
        device: 'desktop',
        return_page_source: 'true',
        block_ads: 'true',
        wait: '2000',
        timeout: '30000'
      });
      creditCost = 25; // Assume worst case
      break;

    default:
      // Final attempt: Stealth proxy (75 credits)
      params = new URLSearchParams({
        api_key: apiKey,
        url: url,
        render_js: 'true',
        stealth_proxy: 'true',
        country_code: 'us',
        device: 'desktop',
        return_page_source: 'true',
        block_ads: 'true',
        wait: '2000',
        timeout: '30000'
      });
      creditCost = 75;
  }

  console.log(`Making request to ScrapingBee API (tier ${attempt}, ~${creditCost} credits)...`);

  try {
    const response = await fetch(`https://app.scrapingbee.com/api/v1/?${params}`);

    console.log(`ScrapingBee response status: ${response.status}`);

    if (!response.ok) {
      const errorText = await response.text();
      console.error(`ScrapingBee error response: ${errorText}`);

      // Try to parse error JSON
      let errorData;
      try {
        errorData = JSON.parse(errorText);
      } catch {
        errorData = { error: errorText };
      }

      // Handle specific error cases
      if (response.status === 403 ||
          (errorData.reason && errorData.reason.includes('403')) ||
          (errorData.error && errorData.error.includes('Server responded with 403'))) {
        // Remember that this domain had issues
        rememberDomainDifficulty(url, attempt + 1);

        if (attempt < 4) {
          console.log(`403 error, trying with higher tier (tier ${attempt + 1})`);
          return crawlPage(url, attempt + 1);
        }
      } else if (response.status === 401) {
        throw new Error('ScrapingBee authentication failed - check API key');
      } else if (response.status === 429) {
        throw new Error('ScrapingBee rate limit exceeded');
      } else if (response.status === 500 ||
                (errorData.error && errorData.error.includes('try again'))) {
        // For 500 errors, also try with higher tier
        rememberDomainDifficulty(url, attempt + 1);

        if (attempt < 4) {
          console.log(`Server error, trying with higher tier (tier ${attempt + 1})`);
          return crawlPage(url, attempt + 1);
        }
      } else if (errorData.error &&
                (errorData.error.includes('premium_proxy') ||
                 errorData.error.includes('render_js'))) {
        // The error message suggests using premium or js rendering
        rememberDomainDifficulty(url, attempt + 1);

        if (attempt < 4) {
          console.log(`API suggests higher tier needed, trying tier ${attempt + 1}`);
          return crawlPage(url, attempt + 1);
        }
      }

      throw new Error(`ScrapingBee error (${response.status}): ${errorData.error || response.statusText}`);
    }

    const html = await response.text();
    const htmlLength = html ? html.length : 0;

    if (htmlLength < 1000 && attempt < 4) {
      // If we got a very small response, it might be a soft block or error page
      console.log(`Warning: Very small HTML response (${htmlLength} chars), might be blocked. Trying higher tier.`);
      rememberDomainDifficulty(url, attempt + 1);
      return crawlPage(url, attempt + 1);
    }

    console.log(`Successfully crawled page: ${url} (received ${htmlLength} characters) using tier ${attempt}`);

    // Remember that this tier worked successfully
    rememberDomainDifficulty(url, attempt);

    return html;

  } catch (error) {
    if (error.message && error.message.includes('rate limit')) {
      throw error; // Don't retry rate limits
    }

    // For other errors, try next tier if not at max
    if (attempt < 4) {
      console.log(`Error on tier ${attempt}, trying higher tier: ${error.message}`);
      return crawlPage(url, attempt + 1);
    }

    throw error;
  }
}

// Fallback function to extract basic info without full crawl
function createFallbackAnalysis(url: string, error: string) {
  console.log('Creating fallback analysis due to crawl failure')
  
  const domain = new URL(url).hostname
  
  return {
    pageTitle: `Page at ${domain}`,
    metaDescription: '',
    mainContent: `Unable to crawl page: ${error}`,
    wordCount: 0,
    imageCount: 0,
    videoPresent: false,
    headingCounts: { h1: 0, h2: 0, h3: 0, h4: 0, h5: 0, h6: 0 },
    totalHeadings: 0,
    hasTable: false,
    tableCount: 0,
    hasUnorderedList: false,
    unorderedListCount: 0,
    hasOrderedList: false,
    orderedListCount: 0,
    schemaMarkupPresent: false,
    ariaLabelsPresent: false,
    internalLinkCount: 0,
    url,
    crawlError: error
  }
}

// Function to extract data from HTML
function extractFromHtml(html: string, url: string) {
  const titleMatch = html.match(/<title[^>]*>([^<]+)<\/title>/i)
  const metaDescMatch = html.match(/<meta[^>]*name=["']description["'][^>]*content=["']([^"']+)["']/i)
  
  const bodyMatch = html.match(/<body[^>]*>([\s\S]*)<\/body>/i)
  const mainContent = bodyMatch ? bodyMatch[1].replace(/<[^>]+>/g, ' ').replace(/\s+/g, ' ').trim() : ''
  
  const wordCount = mainContent.split(/\s+/).length
  const imageCount = (html.match(/<img[^>]*>/gi) || []).length
  const videoPresent = /<video[^>]*>|<iframe[^>]*youtube|<iframe[^>]*vimeo/i.test(html)
  
  const headingCounts = {
    h1: (html.match(/<h1[^>]*>/gi) || []).length,
    h2: (html.match(/<h2[^>]*>/gi) || []).length,
    h3: (html.match(/<h3[^>]*>/gi) || []).length,
    h4: (html.match(/<h4[^>]*>/gi) || []).length,
    h5: (html.match(/<h5[^>]*>/gi) || []).length,
    h6: (html.match(/<h6[^>]*>/gi) || []).length,
  }
  
  const totalHeadings = Object.values(headingCounts).reduce((a, b) => a + b, 0)
  
  const hasTable = /<table[^>]*>/i.test(html)
  const tableCount = (html.match(/<table[^>]*>/gi) || []).length
  const hasUnorderedList = /<ul[^>]*>/i.test(html)
  const unorderedListCount = (html.match(/<ul[^>]*>/gi) || []).length
  const hasOrderedList = /<ol[^>]*>/i.test(html)
  const orderedListCount = (html.match(/<ol[^>]*>/gi) || []).length
  
  const schemaMarkupPresent = /<script[^>]*type=["']application\/ld\+json["'][^>]*>/i.test(html)
  const ariaLabelsPresent = /aria-label=|aria-labelledby=/i.test(html)
  
  const domain = new URL(url).hostname
  const internalLinkRegex = new RegExp(`<a[^>]*href=["'](?:https?:)?\/\/${escapeRegex(domain)}[^"']*["']`, 'gi')
  const internalLinkCount = (html.match(internalLinkRegex) || []).length
  
  return {
    pageTitle: titleMatch ? titleMatch[1].trim() : '',
    metaDescription: metaDescMatch ? metaDescMatch[1].trim() : '',
    mainContent: mainContent.substring(0, 5000),
    wordCount,
    imageCount,
    videoPresent,
    headingCounts,
    totalHeadings,
    hasTable,
    tableCount,
    hasUnorderedList,
    unorderedListCount,
    hasOrderedList,
    orderedListCount,
    schemaMarkupPresent,
    ariaLabelsPresent,
    internalLinkCount,
    url
  }
}

// Function to analyze content using OpenAI
async function analyzeContentWithAI(pageData: any, queryText: string, brandName: string, competitors: any[]) {
  const apiKey = Deno.env.get('OPENAI_API_KEY')
  if (!apiKey) {
    console.error('OpenAI API key not configured, using default analysis')
    return getDefaultAnalysis()
  }

  // Skip AI analysis if we have a crawl error
  if (pageData.crawlError) {
    return getDefaultAnalysis()
  }

  // Find brand and competitor mentions safely
  const brandMentions = findMentionsWithContext(pageData.mainContent || '', brandName || '');
  const competitorAnalysis: any = {};

  // Ensure competitors is an array before iterating
  const competitorsList = Array.isArray(competitors) ? competitors : [];

  for (const competitor of competitorsList) {
    // Check competitor structure is valid
    if (!competitor || typeof competitor !== 'object') {
      console.error('Invalid competitor object:', competitor);
      continue;
    }

    const compName = competitor.name || '';
    if (compName) {
      const mentions = findMentionsWithContext(pageData.mainContent || '', compName);
      competitorAnalysis[compName] = mentions ? mentions.length : 0;
    }
  }

  const prompt = `
Analyze this web page that was cited in response to the query: "${queryText}".

Page info:
URL: ${pageData.url}
Title: ${pageData.pageTitle}
Meta description: ${pageData.metaDescription}
Word count: ${pageData.wordCount}
Brand mentions: ${brandMentions.length}
Competitor mentions: ${JSON.stringify(competitorAnalysis)}
Content sample: ${pageData.mainContent.substring(0, 1000)}

Please provide the following in JSON format:
{
  "content_type": "Blog/Product/Documentation/News/etc",
  "rock_paper_scissors": "Rock/Paper/Scissors",
  "content_depth_score": 1-5,
  "sentiment_score": -1 to 1,
  "content_uniqueness": 1-5,
  "content_optimization_score": 1-5,
  "has_statistics": true/false,
  "has_quotes": true/false,
  "has_research": true/false,
  "analysis_score": 1-5,
  "citation_match_quality": 1-5,
  "topical_cluster": "main topic",
  "page_relevance_type": "direct/partial/misaligned",
  "page_intent_alignment": "high/moderate/low/mismatch",
  "content_format": "article/blog/landing_page/etc",
  "content_depth": "comprehensive/overview/shallow",
  "brand_positioning": "prominent/mentioned/absent",
  "competitor_presence": "exclusive/featured/mentioned/none",
  "call_to_action_strength": "strong/moderate/passive/none",
  "content_recency": "recent/older/outdated/undated",
  "eeat_signals": "strong/moderate/weak",
  "user_experience_quality": "excellent/good/average/poor/problematic",
  "content_structure": "hierarchical/linear/fragmented",
  "analysis_notes": "Brief notes about the page"
}`

  try {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-4-turbo-preview',
        messages: [
          {
            role: 'system',
            content: 'You are an expert content analyst. Provide analysis in valid JSON format.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.3,
        max_tokens: 1000,
        response_format: { type: 'json_object' }
      })
    })

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.statusText}`)
    }

    const data = await response.json()
    const analysis = JSON.parse(data.choices[0].message.content)
    
    return analysis
  } catch (error) {
    console.error('AI analysis error:', error)
    return getDefaultAnalysis()
  }
}

function getDefaultAnalysis() {
  return {
    content_type: "Unknown",
    rock_paper_scissors: "Rock",
    content_depth_score: 3,
    sentiment_score: 0,
    content_uniqueness: 3,
    content_optimization_score: 3,
    has_statistics: false,
    has_quotes: false,
    has_research: false,
    analysis_score: 3,
    citation_match_quality: 3,
    topical_cluster: "General",
    page_relevance_type: "partial",
    page_intent_alignment: "moderate",
    content_format: "unknown",
    content_depth: "shallow",
    brand_positioning: "absent",
    competitor_presence: "none",
    call_to_action_strength: "none",
    content_recency: "undated",
    eeat_signals: "weak",
    user_experience_quality: "average",
    content_structure: "linear",
    analysis_notes: "Default analysis used due to crawl or AI error"
  }
}

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response('ok', { headers: corsHeaders })
  }

  try {
    const body = await req.json() as AnalyzeCitationRequest
    console.log('Received analyze-citation request:', JSON.stringify(body, null, 2))
    
    const { 
      query_id, 
      citation_url, 
      citation_position,
      query_text,
      keyword,
      brand_name,
      brand_domain,
      competitors
    } = body

    if (!citation_url) {
      throw new Error('citation_url is required')
    }

    let extractedData
    
    try {
      // Try to crawl the page with fallback options
      console.log(`Crawling ${citation_url}`)
      const html = await crawlPage(citation_url)
      
      // Extract basic data
      extractedData = extractFromHtml(html, citation_url)
    } catch (crawlError) {
      console.error('Crawl failed, using fallback:', crawlError.message)
      extractedData = createFallbackAnalysis(citation_url, crawlError.message)
    }
    
    // Analyze with AI (will handle fallback if crawl failed)
    console.log('Analyzing content with AI')
    const aiAnalysis = await analyzeContentWithAI(extractedData, query_text, brand_name, competitors)
    
    // Determine if it's client or competitor domain
    const domain = new URL(citation_url).hostname
    const isClientDomain = brand_domain ? domain.includes(brand_domain) : false
    let isCompetitorDomain = false

    // Ensure competitors is an array before processing
    const competitorsList = Array.isArray(competitors) ? competitors : [];

    for (const competitor of competitorsList) {
      if (!competitor || typeof competitor !== 'object') {
        continue;
      }

      const compDomain = competitor.domain || competitor.pattern || '';
      if (compDomain && domain.includes(compDomain)) {
        isCompetitorDomain = true
        break
      }
    }

    // Create service client for Supabase
    const serviceClient = createClient(
      Deno.env.get('SUPABASE_URL') ?? '',
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY') ?? '',
      {
        auth: {
          persistSession: false
        }
      }
    )

    // Create page analysis record
    const pageAnalysis = {
      query_id,
      page_analysis_id: `${domain.replace(/\./g, '_')}_${Date.now()}`,
      citation_url,
      citation_position,
      domain_name: domain,
      is_client_domain: isClientDomain,
      is_competitor_domain: isCompetitorDomain,
      mention_type: ['citation'],
      analysis_notes: aiAnalysis.analysis_notes,
      crawl_status: extractedData.crawlError ? 'failed' : 'success',
      crawl_error: extractedData.crawlError || null,
      
      technical_seo: {
        is_valid: true,
        is_crawlable: !extractedData.crawlError,
        http_response_code: extractedData.crawlError ? 0 : 200,
        schema_markup_present: extractedData.schemaMarkupPresent || false,
        schema_types: [],
        html_structure_score: 3,
        semantic_html_usage: extractedData.totalHeadings > 0,
        mobile_friendly: true,
        hreflang_declaration: false,
        date_created: null,
        date_modified: null,
        cdn_usage: false,
        meta_description_present: !!extractedData.metaDescription,
        aria_labels_present: extractedData.ariaLabelsPresent || false,
        aria_labels_types: [],
        social_graphs_present: false
      },
      
      page_performance: {
        page_speed_score: 50,
        firstContentfulPaint: 1000,
        largestContentfulPaint: 2500,
        totalBlockingTime: 100,
        cumulativeLayoutShift: 0.1,
        accessibility_score: extractedData.ariaLabelsPresent ? 4 : 3
      },
      
      domain_authority: {
        domain_name: domain,
        domain_authority: 30,
        page_authority: 20,
        backlink_count: 100,
        referring_domains: 50,
        link_propensity: 0.5,
        spam_score: 2
      },
      
      on_page_seo: {
        page_title: extractedData.pageTitle || '',
        content_type: aiAnalysis.content_type,
        meta_description: extractedData.metaDescription || '',
        word_count: extractedData.wordCount || 0,
        image_count: extractedData.imageCount || 0,
        video_present: extractedData.videoPresent || false,
        has_table: extractedData.hasTable || false,
        has_table_count: extractedData.tableCount || 0,
        has_unordered_list: extractedData.hasUnorderedList || false,
        has_unordered_list_count: extractedData.unorderedListCount || 0,
        has_ordered_list: extractedData.hasOrderedList || false,
        has_ordered_list_count: extractedData.orderedListCount || 0,
        internal_link_count: extractedData.internalLinkCount || 0,
        folder_depth: citation_url.split('/').length - 3,
        authorship_clear: false,
        heading_count: extractedData.totalHeadings || 0,
        heading_count_type: extractedData.headingCounts || {},
        keyword_match: [keyword]
      },
      
      content_quality: {
        content_depth_score: aiAnalysis.content_depth_score,
        readability_score: 3,
        sentiment_score: aiAnalysis.sentiment_score,
        content_uniqueness: aiAnalysis.content_uniqueness,
        content_optimization_score: aiAnalysis.content_optimization_score,
        has_statistics: aiAnalysis.has_statistics,
        has_quotes: aiAnalysis.has_quotes,
        has_citations: false,
        has_research: aiAnalysis.has_research,
        analysis_score: aiAnalysis.analysis_score,
        rock_paper_scissors: aiAnalysis.rock_paper_scissors,
        citation_match_quality: aiAnalysis.citation_match_quality,
        eeat_score: 3,
        ai_content_detection: 3,
        topical_cluster: aiAnalysis.topical_cluster
      },
      
      page_analysis: {
        page_relevance_type: aiAnalysis.page_relevance_type,
        page_intent_alignment: aiAnalysis.page_intent_alignment,
        content_format: aiAnalysis.content_format,
        content_depth: aiAnalysis.content_depth,
        brand_positioning: aiAnalysis.brand_positioning,
        competitor_presence: aiAnalysis.competitor_presence,
        call_to_action_strength: aiAnalysis.call_to_action_strength,
        content_recency: aiAnalysis.content_recency,
        eeat_signals: aiAnalysis.eeat_signals,
        user_experience_quality: aiAnalysis.user_experience_quality,
        content_structure: aiAnalysis.content_structure
      }
    }

    // Insert into database
    const { data, error } = await serviceClient
      .from('page_analyses')
      .insert(pageAnalysis)
      .select()
      .single()

    if (error) {
      throw new Error(`Database error: ${error.message}`)
    }

    return new Response(
      JSON.stringify({
        success: true,
        page_analysis: data
      }),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 200
      }
    )

  } catch (error) {
    console.error('Citation analysis error:', error)
    
    return new Response(
      JSON.stringify({
        success: false,
        error: error.message
      }),
      {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' },
        status: 400
      }
    )
  }
})